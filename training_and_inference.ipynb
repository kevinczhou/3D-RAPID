{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "This notebook contains code for training a CNN (in one or two steps) and for generating stitched photometric composities/3D height maps via CNN inference. \n",
    "1. Specify the sample and set the optimization round to 1. Make sure `directory` is correct.\n",
    "2. Run each cell in sequence until the CNN checkpoint file is saved. If a second round is necessary, restart the notebook and set the optimization round to 2. \n",
    "3. Run the cells under `Generate and save full reconstructions`, which load the saved CNN checkpoint files and perform CNN inference. This will gradually generate the photometric composite and 3D height map frames.\n",
    "\n",
    "Each section below contains further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'  # restrict GPU usage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "tf.config.experimental.enable_tensor_float_32_execution(False)\n",
    "from mcam3d import mcam3d, flatten_illumination, load_stack\n",
    "from tqdm.notebook import tqdm\n",
    "import xarray as xr\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose sample\n",
    "Set `sample_id` and `optimization_round`. Round 1 registers images using the RGB intensities, while round 2 uses normalized high-pass-filtered versions. Restart the notebook in between rounds. Only the terrestrial organisms (fruit flies and harvester ants) need round 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 'harvester_ants'  # 'zebrafish', 'fruit_flies', or 'harvester_ants'\n",
    "optimization_round = 1  # 1 or 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_id == 'fruit_flies':\n",
    "    directory = '/data/fruit_flies/'\n",
    "    time_points = range(25, 601, 37)  # to make same length as range(0, 601, 40)\n",
    "elif sample_id == 'harvester_ants':\n",
    "    directory = '/data/harvester_ants/'\n",
    "    time_points = range(0, 601, 40)\n",
    "elif sample_id == 'zebrafish':\n",
    "    directory = '/data/zebrafish/'\n",
    "    time_points = range(0, 601, 40)\n",
    "else:\n",
    "    raise Exception('invalid sample_id: ' + sample_id)\n",
    "\n",
    "sample_list = list()\n",
    "for t in time_points:\n",
    "    sample_list.append((os.path.join(directory, 'raw_video.nc'), \n",
    "                        os.path.join(directory, 'calibration2.mat'), t))\n",
    "\n",
    "num_dataset = len(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_id == 'fruit_flies' or sample_id == 'harvester_ants':  # terrestrial samples\n",
    "    if optimization_round == 1:\n",
    "        height_map_reg_coef = 500\n",
    "        hpf_sigmas = None \n",
    "        support_constraint_coef = None\n",
    "        num_iter = 30001\n",
    "        ckpt_path = None\n",
    "    elif optimization_round == 2:\n",
    "        height_map_reg_coef = 50\n",
    "        hpf_sigmas = (1, 2, 4)\n",
    "        support_constraint_coef = 100\n",
    "        num_iter = 70001\n",
    "        ckpt_path = os.path.join(directory, 'CNN_round1')\n",
    "    support_constraint_threshold = 130\n",
    "    rejection_threshold = 130\n",
    "elif sample_id == 'zebrafish':  # aquatic samples\n",
    "    assert optimization_round == 1  # only one round\n",
    "    height_map_reg_coef = 50\n",
    "    hpf_sigmas = None \n",
    "    support_constraint_coef = 100\n",
    "    num_iter = 70001\n",
    "    support_constraint_threshold = 185\n",
    "    rejection_threshold = None\n",
    "    ckt_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-based stitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_and_dataset(im_stack_inds_keep, recon_shape, ul_offset, downsamp, filters_list, \n",
    "                               variable_initial_values, laplace_sigmas=None, gaussian_only=False,\n",
    "                               use_neighboring_cameras=True, skip_list=None, hpf_sigmas=None,\n",
    "                               num_patch=2, patch_size=576, sample_margin=None,  # for making dataset\n",
    "                               visitation_log_margins=None, connectivity=np.ones((1, 3)),\n",
    "                               rejection_threshold=None,  # if None, don't do anything; otherwise, reject batches\n",
    "                               # based on whether it samples a region that meets this threshold\n",
    "                               architecture='fcnn',\n",
    "                              ):\n",
    "    \n",
    "    # create visitation_log at lower scale:\n",
    "    a = mcam3d(stack=im_stack_inds_keep, ul_coords=np.zeros((len(inds_keep), 2)),\n",
    "                recon_shape=recon_shape, ul_offset=ul_offset,\n",
    "                scale=.01*downsamp*binning,\n",
    "                batch_size=None,\n",
    "                momentum=None,\n",
    "                report_error_map=False,\n",
    "               )\n",
    "    a.use_camera_calibration = True\n",
    "    a.effective_focal_length_mm = 25.05486\n",
    "    a.magnification_j = 0.1133\n",
    "    a.create_variables(deformation_model='camera_parameters_perspective_to_orthographic',\n",
    "        learning_rates={'camera_focal_length': -1e-3, 'camera_height': 1e-3, 'ground_surface_normal': 1e-3,\n",
    "                       'camera_in_plane_angle': 1e-3, 'rc': .1, 'gain': -1e-3, 'bias': -1e-3, 'ego_height': 1e-3,\n",
    "                       'radial_camera_distortion': 1e-3},\n",
    "                       variable_initial_values=variable_initial_values,\n",
    "                       remove_global_transform=True, antialiasing_filter=False)\n",
    "    stack_downsamp, rc_downsamp = a.generate_dataset()\n",
    "\n",
    "    loss_i, recon, normalize, error_map, tracked = a.gradient_update(stack_downsamp, rc_downsamp,\n",
    "                                                                     return_tracked_tensors=True, \n",
    "                                                                     update_gradient=False)\n",
    "\n",
    "    # plot recon:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(np.uint8(recon[:,:,1]))\n",
    "    plt.title('low resolution recon')\n",
    "    if rejection_threshold is not None:\n",
    "        good_regions = recon[:,:,1] < rejection_threshold\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(good_regions)\n",
    "        plt.title('good regions')\n",
    "    else:\n",
    "        good_regions = None\n",
    "    plt.show()\n",
    "    \n",
    "    a.recompute_CNN = True  # to save memory\n",
    "\n",
    "    # define network:\n",
    "    a.unet_scale = .0001\n",
    "    if skip_list is None:\n",
    "        skip_list = [0]*len(filters_list)\n",
    "    if connectivity is not None:\n",
    "        num_inputs_to_expanded_stack = connectivity.size\n",
    "    a.define_network_and_camera_params(tracked['vanish_warp'], tracked['camera_to_vanish_point_xyz'], \n",
    "                                       num_channels_rgb=3,\n",
    "                                       architecture=architecture,\n",
    "                                       filters_list=filters_list, \n",
    "                                       skip_list=skip_list, \n",
    "                                       num_inputs_to_expanded_stack=num_inputs_to_expanded_stack,\n",
    "                                       learning_rate=1e-3)\n",
    "\n",
    "    # generate visitation log:\n",
    "    with tf.device('/CPU:0'):\n",
    "        visitation_log_vars = a.generate_visitation_log(camera_margins=visitation_log_margins)\n",
    "\n",
    "    # to allow CNN access to neighboring cameras:\n",
    "    if use_neighboring_cameras:\n",
    "        a.expand_stack_channels_for_CNN(connectivity=connectivity, camera_array_dims=camera_array_dims)\n",
    "        \n",
    "    # postpend channels that will be used for stitching instead of the CNN inputs (or a subset thereof):\n",
    "    if laplace_sigmas is not None:\n",
    "        assert hpf_sigmas is None\n",
    "        a.postpend_edge_filtered_channels_for_registration(im_stack_inds_keep, sigmas=laplace_sigmas,\n",
    "                                                           gaussian_only=gaussian_only\n",
    "                                                          )\n",
    "    if hpf_sigmas is not None:\n",
    "        assert laplace_sigmas is None\n",
    "        a.postpend_edge_filtered_channels_for_registration(im_stack_inds_keep, sigmas=hpf_sigmas, \n",
    "                                                           use_hpf_norm=True)\n",
    "        \n",
    "    # generate dataset\n",
    "    a.visitation_log_scale = a.scale\n",
    "    dataset = a.generate_patched_dataset(num_patch, patch_size, patch_size*2, \n",
    "                                         fracture_big_tensors=True, sample_margin=sample_margin,\n",
    "                                         good_regions=good_regions\n",
    "                                        )\n",
    "        \n",
    "    return a, dataset, visitation_log_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_list = list()  # we're going to merge the datasets\n",
    "downsamp = 1\n",
    "for j, (nc_path, calibration_path, video_frame) in tqdm(enumerate(sample_list), total=len(sample_list)):\n",
    "    # load image stack for one frame:\n",
    "    im_stack = load_stack(nc_path, '', video_frame=video_frame)\n",
    "    \n",
    "    # get precalibrated values:\n",
    "    restored = scipy.io.loadmat(calibration_path)\n",
    "    variable_initial_values = {key:restored[key].squeeze() \n",
    "                               for key in restored if '__' not in key}\n",
    "    variable_initial_values['rc'] = variable_initial_values['rc'] / downsamp\n",
    "    \n",
    "    # get other settings:\n",
    "    inds_keep = restored['inds_keep__'].flatten()\n",
    "    recon_shape = restored['recon_shape__'].flatten()\n",
    "    ul_offset = restored['ul_offset__'].flatten()\n",
    "    camera_array_dims = restored['camera_array_dims__'].flatten()\n",
    "    optimize_illum = restored['optimize_illum__']\n",
    "    if 'binning__' in restored:\n",
    "        binning = restored['binning__'].squeeze()\n",
    "    else:\n",
    "        binning = 1\n",
    "    \n",
    "    if downsamp > 1:\n",
    "        im_stack = im_stack[:, ::downsamp, ::downsamp, :]\n",
    "        recon_shape = recon_shape // downsamp\n",
    "        ul_offset = ul_offset // downsamp\n",
    "\n",
    "    if optimize_illum:\n",
    "        im_stack_inds_keep = flatten_illumination(im_stack, inds_keep, variable_initial_values['illum_flat'])\n",
    "    else:\n",
    "        im_stack_inds_keep = im_stack[inds_keep]\n",
    "\n",
    "    sample_margin = [0, 0, .07, .07]\n",
    "    visitation_log_margins = [.081, 0]  # for individual camera FOVs\n",
    "    print('Bin: ' + str(binning))\n",
    "    if binning == 1:\n",
    "        if 'harvester_ants' in nc_path:\n",
    "            filters_list = np.array([32, 32, 32, 32, 32, 32])\n",
    "        else:\n",
    "            filters_list = np.array([32, 32, 32, 32, 32])\n",
    "        skip_list = np.array([0] * len(filters_list))\n",
    "        patch_size = 1024\n",
    "        num_patch = 1\n",
    "    elif binning == 4:\n",
    "        if 'harvester_ants' in nc_path:\n",
    "            filters_list = np.array([32, 32, 32, 32])\n",
    "        else:\n",
    "            filters_list = np.array([32, 32, 32])\n",
    "        skip_list = np.array([0] * len(filters_list))\n",
    "        patch_size = 384\n",
    "        num_patch = 8\n",
    "    elif binning == 2:\n",
    "        if 'harvester_ants' in nc_path:\n",
    "            filters_list = np.array([32, 32, 32, 32, 32])\n",
    "        else:\n",
    "            filters_list = np.array([32, 32, 32, 32])\n",
    "        skip_list = np.array([0] * len(filters_list))\n",
    "        patch_size = 768\n",
    "        num_patch = 2\n",
    "    else:\n",
    "        raise Exception('binning must be 1, 2,  or 4')\n",
    "\n",
    "    laplace_sigmas = None\n",
    "    assert (laplace_sigmas is None) + (hpf_sigmas is None) != 0  # only one of these can be chosen\n",
    "\n",
    "    with tf.device('/CPU:0'):\n",
    "        a, dataset, visitation_log_vars = generate_model_and_dataset(im_stack_inds_keep, recon_shape, ul_offset, \n",
    "                                                                     downsamp,\n",
    "                                                                     filters_list,\n",
    "                                                                     variable_initial_values,\n",
    "                                                                     hpf_sigmas=hpf_sigmas,\n",
    "                                                                     laplace_sigmas=laplace_sigmas,\n",
    "                                                                     gaussian_only=False,\n",
    "                                                                     num_patch=num_patch, patch_size=patch_size,\n",
    "                                                                     sample_margin=sample_margin,\n",
    "                                                                     skip_list=skip_list,\n",
    "                                                                     visitation_log_margins=visitation_log_margins,\n",
    "                                                                     rejection_threshold=rejection_threshold,\n",
    "                                                                    )\n",
    "    dataset_list.append(dataset)\n",
    "\n",
    "plt.imshow((visitation_log_vars[0].numpy()>0).sum(2))\n",
    "plt.title('visitation')\n",
    "plt.show()\n",
    "    \n",
    "# combine all the datasets (i.e., cycle through all of them):\n",
    "choice_dataset = tf.data.Dataset.range(num_dataset).repeat()\n",
    "dataset = tf.data.experimental.choose_from_datasets(dataset_list, choice_dataset)\n",
    "    \n",
    "# restore checkpointed network:\n",
    "if optimization_round == 2:\n",
    "    a.ckpt = None\n",
    "    a.checkpoint_all_variables(ckpt_path, skip_saving=True)\n",
    "    a.restore_all_variables(ckpt_no=0)\n",
    "    a.ckpt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ii = 0\n",
    "losses = list()\n",
    "for batch in tqdm(dataset, total=num_iter):\n",
    "    if any([len(batch[0][i]) < 2 for i in range(num_patch)]):\n",
    "        # there needs to be at least two patches to register!\n",
    "        continue\n",
    "\n",
    "    loss_i, recon_i, normalize_i, grads_i, norm_i = a.gradient_update_patch(\n",
    "        batch, height_map_reg_coef=height_map_reg_coef,\n",
    "        return_gradients=True,\n",
    "        clip_gradient_norm=10,\n",
    "        dither_coords=True,\n",
    "        downsample_factor=1,\n",
    "        support_constraint_coef=support_constraint_coef,\n",
    "        support_constraint_threshold=support_constraint_threshold,\n",
    "    )\n",
    "    losses.append([l.numpy() for l in loss_i])\n",
    "    \n",
    "    if len(losses) % 100 == 0:\n",
    "        print(len(losses), 'Loss terms: ' + str(losses[-1]))\n",
    "        \n",
    "    if len(losses) % 2000 == 0 or len(losses) == 1:\n",
    "        plt.plot(losses)\n",
    "        plt.title('loss history')\n",
    "        plt.show()\n",
    "        \n",
    "        if len(losses) > 51:\n",
    "            if type(loss_i) is list:\n",
    "                plt.plot(np.convolve(np.array([loss[0] for loss in losses][50:]), np.ones(2000)/2000, 'valid'))\n",
    "            else:\n",
    "                plt.plot(np.convolve(np.array(losses[50:]).squeeze(), np.ones(2000)/2000, 'valid'))\n",
    "            plt.title('blurred loss history')\n",
    "            plt.show()\n",
    "        \n",
    "        plt.subplot(121)\n",
    "        plt.imshow(np.uint8(recon_i[:,:,0, :-1]))\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(recon_i[:,:,0, -1], cmap='jet')\n",
    "        plt.show()\n",
    "        \n",
    "    if len(losses) % 10000 == 0:\n",
    "        # generate full reconstruction:\n",
    "        with tf.device('/CPU:0'):\n",
    "            if binning == 1:\n",
    "                margin = 30\n",
    "            elif binning == 4:\n",
    "                margin = 6\n",
    "            elif binning == 2:\n",
    "                margin = 12\n",
    "                \n",
    "            recon_full, _ = a.generate_full_recon(margin=margin)\n",
    "            recon_full = recon_full.numpy()\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(recon_full[:, :, :-1].astype(np.uint8))\n",
    "            plt.title('photometric recon')\n",
    "            plt.show()\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(recon_full[:, :, -1], cmap='turbo')\n",
    "            plt.title('height map recon')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure(figsize=(15,15))\n",
    "            plt.imshow(recon_full[3000//binning:5000//binning, 4500//binning:6500//binning, :3].astype(np.uint8), cmap='turbo')\n",
    "            plt.title('crop of photometric recon')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure(figsize=(15,15))\n",
    "            plt.imshow(recon_full[3000//binning:5000//binning, 4500//binning:6500//binning, -1], cmap='turbo')\n",
    "            plt.title('crop of height map recon')\n",
    "            plt.show()\n",
    "\n",
    "    if ii == num_iter:\n",
    "        break\n",
    "    else:\n",
    "        ii+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save CNN checkpoint file in a unique directory:\n",
    "a.ckpt = None\n",
    "if optimization_round == 1:\n",
    "    ckpt_path = os.path.join(directory, 'CNN_round1')\n",
    "elif optimization_round == 2:\n",
    "    ckpt_path = os.path.join(directory, 'CNN_round2')\n",
    "print(ckpt_path)\n",
    "a.checkpoint_all_variables(path=ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the notebook after round 1 and run round 2, if necessary. Otherwise, continue to next step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save full reconstructions\n",
    "After optimizing the CNN (both rounds if applicable), run inference on the CNN to generate full videos here. If running from a reset notebook, run all cells until `generate_model_and_dataset` is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binning = 2\n",
    "nc_path = os.path.join(directory, 'raw_video.nc')\n",
    "calibration_path = os.path.join(directory, 'calibration2.mat')\n",
    "new_directory = os.path.join(directory, '3D_video_frames')  # where to save video frames\n",
    "video_slice_range = np.arange(601)  # which frames to reconstruct\n",
    "save_float32_copy = False  # script will save rgb and height maps as 3-channel uint8 images;\n",
    "# if you want a float32 copy of the height map in mm, set sae_float32_copy to True\n",
    "\n",
    "if sample_id == 'zebrafish':\n",
    "    ckpt_path = os.path.join(directory, 'CNN_round1')  # trained network\n",
    "    height_clims = [-1.4, 1.4]  # height range in mm\n",
    "    color_corr = np.array([1.1284974, 1.1000000, 1.3200001], \n",
    "                          dtype=np.float32)  # correct the RGB color imbalance\n",
    "elif sample_id == 'harvester_ants':\n",
    "    ckpt_path = os.path.join(directory, 'CNN_round2')\n",
    "    height_clims = [0, 4.5]\n",
    "    color_corr = np.array([1.3615384, 1.1000000, 1.6225001], \n",
    "                          dtype=np.float32)\n",
    "elif sample_id == 'fruit_flies':\n",
    "    ckpt_path = os.path.join(directory, 'CNN_round2')\n",
    "    height_clims = [0, 4]\n",
    "    color_corr = np.array([1.3597223, 1.1000000, 1.6316667], \n",
    "                          dtype=np.float32)\n",
    "\n",
    "if binning == 1:\n",
    "    if 'harvester_ants' in nc_path:\n",
    "        filters_list = np.array([32, 32, 32, 32, 32, 32])\n",
    "    else:\n",
    "        filters_list = np.array([32, 32, 32, 32, 32])\n",
    "    device = '/CPU:0'  # OOM error if GPU not big enough\n",
    "    margin = 20\n",
    "    patch_size = 1024\n",
    "elif binning == 2:\n",
    "    if 'harvester_ants' in nc_path:\n",
    "        filters_list = np.array([32, 32, 32, 32, 32])\n",
    "    else:\n",
    "        filters_list = np.array([32, 32, 32, 32])\n",
    "    device = '/GPU:0'\n",
    "    margin = 10\n",
    "    patch_size = 768\n",
    "elif binning == 4:\n",
    "    if 'harvester_ants' in nc_path:\n",
    "        filters_list = np.array([32, 32, 32, 32])\n",
    "    else:\n",
    "        filters_list = np.array([32, 32, 32])\n",
    "    device = '/GPU:0'\n",
    "    margin = 5\n",
    "    patch_size = 384\n",
    "\n",
    "num_patch = 1\n",
    "downsamp = 1\n",
    "sample_list = list()\n",
    "for i in video_slice_range:\n",
    "    sample_list.append((nc_path, calibration_path, i))\n",
    "\n",
    "num_dataset = len(sample_list)\n",
    "\n",
    "# make new directory to save these images:\n",
    "os.makedirs(new_directory, exist_ok=True)\n",
    "savename_base = os.path.join(new_directory, 'stitched_rgb')\n",
    "height_savename_base = os.path.join(new_directory, 'height_map')\n",
    "\n",
    "# colormap object:\n",
    "turbo_color = matplotlib.cm.get_cmap('turbo')(np.arange(256))[:, :-1]  # get look up table values\n",
    "# linearly interpolate between 256 colors (default behavior doesn't interpolate):\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('turbo_interp', turbo_color, N=2**16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# almost same script as for generating the tf.datasets, modified to just generate reconstructions:\n",
    "for j, (nc_path, calibration_path, video_frame) in tqdm(enumerate(sample_list), total=len(sample_list)):\n",
    "    # load image stack for one frame:\n",
    "    im_stack = load_stack(nc_path, '', video_frame=video_frame)\n",
    "    \n",
    "    if downsamp > 1:\n",
    "        im_stack = im_stack[:, ::downsamp, ::downsamp, :]\n",
    "        recon_shape = recon_shape // downsamp\n",
    "        ul_offset = ul_offset // downsamp\n",
    "        x_pos = x_pos / downsamp\n",
    "        y_pos = y_pos / downsamp\n",
    "    \n",
    "    # get precalibrated values:\n",
    "    restored = scipy.io.loadmat(calibration_path)\n",
    "    variable_initial_values = {key:restored[key].squeeze() \n",
    "                               for key in restored if '__' not in key}\n",
    "    variable_initial_values['rc'] = variable_initial_values['rc'] / downsamp\n",
    "    \n",
    "    # get other settings:\n",
    "    inds_keep = restored['inds_keep__'].flatten()\n",
    "    recon_shape = restored['recon_shape__'].flatten()\n",
    "    ul_offset = restored['ul_offset__'].flatten()\n",
    "    camera_array_dims = restored['camera_array_dims__'].flatten()\n",
    "    optimize_illum = restored['optimize_illum__']\n",
    "\n",
    "    if optimize_illum:\n",
    "        im_stack_inds_keep = flatten_illumination(im_stack, inds_keep, variable_initial_values['illum_flat'])\n",
    "        print('optimized illumination')\n",
    "    else:\n",
    "        im_stack_inds_keep = im_stack[inds_keep]\n",
    "\n",
    "    with tf.device(device):\n",
    "        a, dataset, visitation_log_vars = generate_model_and_dataset(im_stack_inds_keep, recon_shape, ul_offset, \n",
    "                                                                     downsamp,\n",
    "                                                                     filters_list,\n",
    "                                                                     variable_initial_values,\n",
    "                                                                     num_patch=num_patch, patch_size=patch_size\n",
    "                                                                    )\n",
    "\n",
    "        # restore old network:\n",
    "        a.ckpt = None\n",
    "        a.checkpoint_all_variables(ckpt_path,\n",
    "                                   skip_saving=True)\n",
    "        a.restore_all_variables(ckpt_no=0)\n",
    "        a.ckpt = None\n",
    "        \n",
    "        # generate reconstruction:\n",
    "        recon_full, normalize = a.generate_full_recon(margin=margin)\n",
    "        recon_full = recon_full.numpy()\n",
    "        normalize = normalize.numpy()\n",
    "        \n",
    "        if j == 0:\n",
    "            # keep a fixed color range:\n",
    "            clims = height_clims\n",
    "            \n",
    "            # crop out regions of zeros:\n",
    "            r_nonzero, c_nonzero = np.nonzero(recon_full[:, :, -1])\n",
    "            r0 = r_nonzero.min()\n",
    "            r1 = r_nonzero.max()\n",
    "            c0 = c_nonzero.min()\n",
    "            c1 = c_nonzero.max()\n",
    "            \n",
    "        # crop margins:\n",
    "        recon_cropped = recon_full[r0:r1, c0:c1].copy()\n",
    "        normalize_cropped = normalize[r0:r1, c0:c1].copy()\n",
    "        \n",
    "        # generate height map:\n",
    "        height_map = recon_cropped[:, :, -1]\n",
    "            \n",
    "        # names for saving:\n",
    "        savename = savename_base + '_' + f'{j:04}' + '.png'\n",
    "        height_savename = height_savename_base + '_' + f'{j:04}' + '.png'\n",
    "        height_savename_f32 = height_savename_base + '_' + f'{j:04}' + '.mat'\n",
    "        \n",
    "        # save height map as float32 before casting down to uint8:\n",
    "        if save_float32_copy:\n",
    "            scipy.io.savemat(height_savename_f32, {'height_map': height_map})\n",
    "        \n",
    "        # cast down to save as RGB images:\n",
    "        recon = np.uint8(np.clip(recon_cropped[:, :, :-1] * color_corr[None, None, :], 0, 255))\n",
    "        height_map = np.clip(height_map, clims[0], clims[1])\n",
    "        \n",
    "        # save reconstruction:\n",
    "        plt.imsave(savename, recon)\n",
    "        \n",
    "        # convert height map to RGB for saving:\n",
    "        height_map -= clims[0]\n",
    "        height_map /= (clims[1] - clims[0])\n",
    "        height_map = (255*cmap(height_map)).astype(np.uint8)  # converted to rgb\n",
    "        plt.imsave(height_savename, height_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
